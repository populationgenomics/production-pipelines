[workflow]
name = 'rd_combiner'
status_reporter = 'metamist'

## If set, stop at generation of the joint-callset using the Combiner
#last_stages = ['CreateDenseMtFromVdsWithHail']

## if this is False, we will not check for existing VDS - always create fresh VDS from gVCFs
check_for_existing_vds = false

## If this is populated with an integer, we will use the VDS with this ID in Metamist as the starting point
#use_specific_vds = 1234

[rd_combiner]
# these are used when calculating how many fragments to send to each job
vqsr_training_fragments_per_job = 100
vqsr_apply_fragments_per_job = 60
indel_recal_disc_size = 20
snps_gather_disc_size = 10
snps_recal_disc_size = 20

# choices of jar specs to use for the various steps
# by default we match the native Hail version in the container
[rd_combiner.subset]
jar_spec_revision = false

[rd_combiner.annotate_cohort]
jar_spec_revision = false

[rd_combiner.annotate_dataset]
jar_spec_revision = false

[rd_combiner.densify]
jar_spec_revision = false

[combiner]
memory = "8Gi"
storage = "10Gi"

[vqsr]
# VQSR, when applying model, targets indel_filter_level and snp_filter_level
# sensitivities. The tool matches them internally to a VQSLOD score cutoff
# based on the model's estimated sensitivity to a set of true variants.
snp_filter_level = 99.7
indel_filter_level = 99.0

[gcloud_condense]
chunk_size = 32
storage = "10Gi"

[references]
# this might be a holdover until the references file is corrected. Working in Main.
vep_110_mount = "gs://cpg-common-main/references/vep/110/mount"
