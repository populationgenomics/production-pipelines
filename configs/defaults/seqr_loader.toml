[workflow]
name = 'seqr_loader'
dataset_gcp_project = 'seqr-308602'
dataset = 'seqr'
status_reporter = 'metamist'

# Use GnarlyGenotyper instead of GenotypeGVCFs
use_gnarly = false

# Use allele-specific annotations for VQSR
use_as_vqsr = true

# Realign CRAM when available, instead of using FASTQ.
# The parameter value should correspond to CRAM version
# (e.g. v0 in gs://cpg-fewgenomes-main/cram/v0/CPG01234.cram
#realign_from_cram_version = 'v0'

# Calling intervals (defauls to whole genome intervals)
#intervals_path =

# Create Seqr ElasticSearch indices for these datasets. If not specified, will
# create indices for all input datasets.
#create_es_index_for_datasets = []

write_vcf = ["udn-aus"]

[resource_overrides]
# Override default resource requirements for unusually large seq data without
# demanding higher resources for all operations as standard. Examples below

# picard MarkDuplicates overrides for unreasnobly large sequnce groups
#picard_mem_gb = 100
#picard_storage_gb = 350

# haplotype caller overrides, see production-pipelines PR#381
# defaults in code are 40 for genomes, none for exomes
#haplotypecaller_storage = 80

[vqsr]
# VQSR, when applying model, targets indel_filter_level and snp_filter_level
# sensitivities. The tool matches them internally to a VQSLOD score cutoff
# based on the model's estimated sensitivity to a set of true variants.
snp_filter_level = 99.7
indel_filter_level = 99.0

[cramqc]
assume_sorted = true
num_pcs = 4

[qc_thresholds.genome.min]
"MEDIAN_COVERAGE" = 10
"PCT_PF_READS_ALIGNED" = 0.80
[qc_thresholds.genome.max]
"FREEMIX" = 0.04
"PERCENT_DUPLICATION" = 25

[hail]
pool_label = 'seqr'
billing_project = 'seqr'

[slack]
channel = 'workflows-qc'
token_secret_id = 'slack-seqr-loader-token'
token_project_id = 'seqr-308602'

[elasticsearch]
# Configure access to ElasticSearch server
port = '9243'
host = 'elasticsearch.es.australia-southeast1.gcp.elastic-cloud.com'
username = 'seqr'
# Load ElasticSearch password from a secret, unless SEQR_ES_PASSWORD is set
password_secret_id = 'seqr-es-password'
password_project_id = 'seqr-308602'

[stripy]
# Analysis_type can be "standard" (fast) or "extended" (marginally slower
# but also uses unmapped reads for genotying)
analysis_type = "extended"
# See https://gitlab.com/andreassh/stripy-pipeline#list-of-loci
# Excluded by default: C9orf72, HTT
target_loci = """AFF2,AR,ARX_1,ARX_2,ATN1,ATXN1,ATXN10,ATXN2,ATXN3,ATXN7,ATXN8OS,\
BEAN1,CACNA1A,CBL,CNBP,COMP,DAB1,DIP2B,DMD,DMPK,FMR1,FOXL2,FXN,GIPC1,GLS,\
HOXA13_1,HOXA13_2,HOXA13_3,HOXD13,JPH3,LRP12,MARCHF6,NIPA1,NOP56,NOTCH2NLC,\
NUTM2B-AS1,PABPN1,PHOX2B,PPP2R2B,PRDM12,RAPGEF2,RFC1,RUNX2,SAMD12,SOX3,STARD7,TBP,\
TBX1,TCF4,TNRC6A,XYLT1,YEATS2,ZIC2,ZIC3"""

[mito_snv]
# Example config for broad wdl found here:
# https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/mitochondria_m2_wdl/ExampleInputsMitochondriaPipeline.json
# f_score_beta is not configured so will use tool default of 1.0
f_score_beta = 1.0
# Sarah Stenton from Broad runs this pipline for seqr ingest and indicated they use a
# threshold of 0.01 for seqr cohorts.
vaf_filter_threshold = 0.01
# Use verifybamid in adition to haplocheck for contamination estimate
# Broad pipeline is not clear if this option is used by default?
use_verifybamid = true
