[workflow]
name = 'large_cohort'
scatter_count = 50
status_reporter = 'metamist'

# Realign CRAM when available, instead of using FASTQ.
# The parameter value should correspond to CRAM version
# (e.g. v0 in gs://cpg-fewgenomes-main/cram/v0/CPG01234.cram
#realign_from_cram_version = 'v0'

# Calling intervals (defauls to whole genome intervals)
#intervals_path =

[vqsr]
# VQSR, when applying model, targets indel_filter_level and snp_filter_level
# sensitivities. The tool matches them internally to a VQSLOD score cutoff 
# based on the model's estimated sensitivity to a set of true variants.
snp_filter_level = 99.7
indel_filter_level = 99.0

[large_cohort]
# minimum random forest probability for population assignment
min_pop_prob = 0.5
# maximum kin threshold to be considered unrelated
max_kin = 0.2
# number of principal components
n_pcs = 16
# metamist `Participant/meta` field to get known ancestry label for RF training
pop_meta_field = 'Superpopulation name'

[large_cohort.sample_qc_cutoffs]
min_coverage = 18
max_n_snps = 8000000
min_n_snps = 2400000
max_n_singletons = 800000
max_r_duplication = 0.3
max_r_het_hom = 3.3

[hail]
pool_label = 'large-cohort'
delete_scratch_on_exit = false

# Autoscaling policy must be created in the project that corresponds 
# to the analysis dataset.
#[hail.dataproc]
#combiner_autoscaling_policy = 'vcf-combiner-50'

[references.gnomad]
tel_and_cent_ht = "gs://cpg-common-main/references/gnomad/v0/telomeres_and_centromeres/hg38.telomeresAndMergedCentromeres.ht"
predetermined_qc_variants = "gs://cpg-common-main/references/gnomad/v0/sample_qc/pre_ld_pruning_qc_variants.ht"

[slack]
channel = 'workflows-qc'
token_secret_id = 'slack-seqr-loader-token'
token_project_id = 'seqr-308602'

[qc_thresholds.genome.min]
"MEDIAN_COVERAGE" = 10
"PCT_PF_READS_ALIGNED" = 0.80
[qc_thresholds.genome.max]
"FREEMIX" = 0.04
"PERCENT_DUPLICATION" = 25
