[workflow]
name = 'large_cohort'
scatter_count_genotype = 50
status_reporter = 'metamist'
# define reference fasta. If not specified here, the reference file is pulled from [references.broad][ref_fasta]
ref_fasta='gs://cpg-common-main/references/hg38/v0/dragen_reference/Homo_sapiens_assembly38_masked.fasta'

# Realign CRAM when available, instead of using FASTQ.
# The parameter value should correspond to CRAM version
# (e.g. v0 in gs://cpg-fewgenomes-main/cram/v0/CPG01234.cram
#realign_from_cram_version = 'v0'

# Calling intervals (defauls to whole genome intervals)
#intervals_path =

[vqsr]
# VQSR, when applying model, targets indel_filter_level and snp_filter_level
# sensitivities. The tool matches them internally to a VQSLOD score cutoff 
# based on the model's estimated sensitivity to a set of true variants.
snp_filter_level = 99.7
indel_filter_level = 99.0

[large_cohort]
# Parameters for reladness inference
max_kin = 0.2  # maximum kin threshold to be considered unrelated in relatedness check (second degree)
# As in hail docs https://hail.is/docs/0.2/methods/relatedness.html, 0.25
# Parameters for population inference
n_pcs = 16  # number of principal components for PCA analysis
min_pop_prob = 0.5  # minimum random forest probability for population assignment
pop_meta_field = 'Superpopulation name'  # metamist's `Participant/meta` field for the
                                         # known population label for the RF training
# whether or not to key samples by their external ID
use_external_id = false
pca_plot_name = 'hgdp_1kg_prophecy_sites'

# Section that specifies background samples for PCA analysis.
# `datasets` must be a list paths to matrix tables or VDS, and 
# `pop_field` if defined would specify the column-level field in th datasets
# to extract the population tag for RF training for ancestry inference.
[large_cohort.pca_background]
#datasets = ['gs://cpg-common-main/references/ancestry/oceania_eur.mt']
#pop_field = 'continental_pop'
# set whether PCA output labels are from given population names, or random forest
# inferred population labels
inferred_ancestry = false

# Parameters for sample QC. The default values for soft filtering taken from gnomAD QC:
# https://macarthurlab.org/2019/10/16/gnomad-v3-0, with `max_n_snps` and 
# `max_n_singletons` adjusted based on the results we got for the TOB-WGS project.
[large_cohort.sample_qc_cutoffs]
min_coverage = 18  # minimum mean coverage for a sample
max_n_snps = 8000000  # maximum number of SNPs for a sample
min_n_snps = 2400000  # minimum number of SNPs for a sample
max_n_singletons = 800000  # maximum number of unique SNPs for a sample
max_r_duplication = 0.3  # maximum rate of duplicated reads (from picard metrics)
max_r_het_hom = 3.3  # maximum rate of heterozygous to homozygous SNPs

[large_cohort.cram_qc]
assume_sorted = true
num_pcs = 4

[hail]
pool_label = 'large-cohort'
delete_scratch_on_exit = false

# Autoscaling policy must be created in the project that corresponds 
# to the analysis dataset.
#[hail.dataproc]
#combiner_autoscaling_policy = 'vcf-combiner-50'

[references.gnomad]
tel_and_cent_ht = "gs://cpg-common-main/references/gnomad/v0/telomeres_and_centromeres/hg38.telomeresAndMergedCentromeres.ht"
predetermined_qc_variants = "gs://cpg-prophecy-main/sites_table/v1-1/pruned_variants.ht/"
