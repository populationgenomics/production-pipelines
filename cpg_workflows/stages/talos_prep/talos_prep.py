"""
This workflow is designed to generate a minimally annotated dataset for use in Talos
this workflow is designed to be something which could be executed easily off-site by non-CPG users

* Step 1: Read a MatrixTable generated by AnnotateDataset
    - Strip out all the prior annotation content
    - Apply a region-filtered based on a provided BED file (only genic regions currently relevant to Talos)
    - Write this minimised MatrixTable to temporary storage
    - Write a sites-only VCF representation in fragments (WAY faster than a serial export)
    - Data is written into tmp

* Step 2: Reassemble the sites-only VCF fragments into a single VCF
    - This uses the gcloud compose operation, which is a rolling merge of the fragments
    - Code re-used from the rd_combiner workflow
    - Data is written into permanent storage, it's only small

* Step 3: Annotate the sites-only VCF with gnomad frequencies
    - This uses Echtvar (https://github.com/populationgenomics/images/tree/main/images/echtvar)
    - Applies a pre-extracted subset of echtvar annotations to the VCF
    - Data is written into tmp

* Step 4: Annotate the gnomAD-annotated VCF with per-transcript consequences
    - Uses bcftools csq, with a GFF3 file and reference genome
    - Data is written into tmp

* Step 5: Process the annotated VCF into a HailTable
    - Loads up the annotated VCF into a Hail Table
    - Splits up the chunky BCSQ annotations into a Hail Array of Hail Structs
    - Adds in per-transcript annotation from MANE and AlphaMissense
    - Saves the final object as a HailTable
    - Data is written into tmp

* Step 6: Load the HailTable into the final MatrixTable
    - Reads the minimised MatrixTable and the HailTable of annotations

* Step 7: Compress the final MatrixTable into a tarball
    - Localise the MatrixTable inside the container using gcloud
    - Compress the MatrixTable into a tarball using zstd
    - Data is written into permanent storage, and registered into Metamist
"""

from functools import cache

from cpg_utils import Path
from cpg_utils.config import config_retrieve, image_path, reference_path
from cpg_workflows.jobs.gcloud_composer import gcloud_compose_vcf_from_manifest
from cpg_workflows.stages.talos import query_for_latest_hail_object
from cpg_workflows.targets import Dataset
from cpg_workflows.utils import exists, get_logger
from cpg_workflows.workflow import DatasetStage, StageInput, StageOutput, get_batch, get_workflow, stage

SHARD_MANIFEST = 'shard-manifest.txt'


@cache
def does_final_file_path_exist(dataset: Dataset) -> bool:
    """
    This workflow includes the generation of a lot of temporary files and folders
    If I run it again, I don't want to accidentally regenerate all the intermediates, even though the final output
    exists. In that scenario I just want no jobs to be planned.

    This method builds the path to the final object, and checks if it exists in GCP
    If it does, we can skip all other stages

    Args:
        dataset (Dataset):

    Returns:
        bool, whether the final file in the workflow already exists
    """
    # if the name of the SquashMtIntoTarball Stage changes, update this String
    path = get_workflow().prefix / 'SquashMtIntoTarball' / f'{dataset.name}.mt.tar'
    return exists(path)


@stage
class ExtractDataFromDatasetMt(DatasetStage):
    """
    extract some plain calls from a joint-callset
    these calls are a region-filtered subset, limited to genic regions
    """

    def expected_outputs(self, dataset: Dataset) -> dict[str, Path]:

        return {
            # write path for the full (region-limited) MatrixTable, stripped of info fields
            'mt': self.tmp_prefix / f'{dataset.name}.mt',
            # this will be the write path for fragments of sites-only VCF
            'sites_only_vcf_dir': str(self.tmp_prefix / f'{dataset.name}_separate.vcf.bgz'),
            # this will be the file which contains the name of all fragments
            'sites_only_vcf_manifest': self.tmp_prefix / f'{dataset.name}_separate.vcf.bgz' / SHARD_MANIFEST,
        }

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:
        """
        script is called extract_vcf_from_mt
        """
        outputs = self.expected_outputs(dataset)

        if does_final_file_path_exist(dataset):
            get_logger().info(f'Skipping {self.name} for {dataset.name}, final workflow output already exists')
            return self.make_outputs(dataset, outputs, jobs=[])

        # either get a mt from metamist, or take one from config
        if (
            input_mt := query_for_latest_hail_object(
                dataset=dataset.name,
                analysis_type='matrixtable',
                object_suffix='.mt',
            )
        ) is None:
            raise ValueError(f'No MatrixTable found in Metamist for {dataset.name}')

        # get the BED file - does not need to be localised
        ensembl_version = config_retrieve(['workflow', 'ensembl_version'], 113)
        bed = reference_path(f'ensembl_{ensembl_version}/merged_bed')

        job = get_batch().new_job(f'ExtractDataFromCohortMt: {dataset.name}')
        job.storage('10Gi')
        job.image(config_retrieve(['workflow', 'driver_image']))
        job.command(
            f"""
            extract_vcf_from_mt \
            --input {input_mt} \
            --output_mt {str(outputs['mt'])} \
            --output_sites_only {str(outputs['sites_only_vcf_dir'])} \
            --bed {bed}
            """,
        )

        return self.make_outputs(dataset, outputs, jobs=job)


@stage(required_stages=ExtractDataFromDatasetMt)
class ConcatenateSitesOnlyVcfFragments(DatasetStage):
    """
    glue all the mini-VCFs together
    """

    def expected_outputs(self, dataset: Dataset) -> Path:
        return self.prefix / f'{dataset.name}_sites_only_reassembled.vcf.bgz'

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:
        """
        trigger a rolling merge using gcloud compose, gluing all the individual files together
        """

        output = self.expected_outputs(dataset)

        sites_manifest = inputs.as_dict(dataset, ExtractDataFromDatasetMt)['sites_only_vcf_manifest']

        if not sites_manifest.exists():
            raise ValueError(
                f'Manifest file {str(sites_manifest)} does not exist, '
                f'run the rd_combiner workflow with workflows.last_stages=[ExtractVcfFromCohortMt]',
            )

        jobs = gcloud_compose_vcf_from_manifest(
            manifest_path=sites_manifest,
            intermediates_path=str(self.tmp_prefix / dataset.name / 'sites_temporary_compose_intermediates'),
            output_path=str(output),
            job_attrs={'stage': self.name},
            final_size='10Gi',
            create_index=False,
        )
        return self.make_outputs(dataset, data=output, jobs=jobs)


@stage(required_stages=ConcatenateSitesOnlyVcfFragments)
class AnnotateGnomadFrequenciesWithEchtvar(DatasetStage):
    """
    Annotate this cohort joint-call VCF with gnomad frequencies, write to tmp storage
    """

    def expected_outputs(self, dataset: Dataset) -> Path:
        return self.tmp_prefix / f'{dataset.name}_gnomad_frequency_annotated.vcf.bgz'

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:
        outputs = self.expected_outputs(dataset)

        if does_final_file_path_exist(dataset):
            get_logger().info(f'Skipping {self.name} for {dataset.name}, final workflow output already exists')
            return self.make_outputs(dataset, outputs, jobs=[])

        sites_vcf = get_batch().read_input(str(inputs.as_path(dataset, ConcatenateSitesOnlyVcfFragments)))

        # this is a single whole-genome file, generated by the echtvar workflow
        gnomad_annotations = get_batch().read_input(config_retrieve(['annotations', 'echtvar_gnomad']))

        job = get_batch().new_job(f'AnnotateGnomadFrequenciesWithEchtvar: {dataset.name}')
        job.image(image_path('echtvar'))
        job.command(f'echtvar anno -e {gnomad_annotations} {sites_vcf} {job.output}')
        job.storage('20Gi')
        job.memory('highmem')
        job.cpu(4)

        get_batch().write_output(job.output, str(outputs))

        return self.make_outputs(dataset, data=outputs, jobs=job)


@stage(required_stages=AnnotateGnomadFrequenciesWithEchtvar)
class AnnotateConsequenceWithBcftools(DatasetStage):
    """
    Take the VCF with gnomad frequencies, and annotate with consequences using BCFtools
    Writes into a cohort-specific permanent folder
    """

    def expected_outputs(self, dataset: Dataset) -> Path:
        return self.tmp_prefix / f'{dataset.name}_consequence_annotated.vcf.bgz'

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:
        output = self.expected_outputs(dataset)

        if does_final_file_path_exist(dataset):
            get_logger().info(f'Skipping {self.name} for {dataset.name}, final workflow output already exists')
            return self.make_outputs(dataset, output, jobs=[])

        gnomad_annotated_vcf = get_batch().read_input(
            str(
                inputs.as_path(dataset, AnnotateGnomadFrequenciesWithEchtvar),
            ),
        )

        # get the GFF3 file required to generate consequences
        ensembl_version = config_retrieve(['workflow', 'ensembl_version'], 113)
        gff3_file = get_batch().read_input(reference_path(f'ensembl_{ensembl_version}/gff3'))

        # get the fasta
        fasta = get_batch().read_input(reference_path('broad/ref_fasta'))

        job = get_batch().new_job(f'AnnotateConsequenceWithBcftools: {dataset.name}')
        job.image(image_path('bcftools_120'))
        job.cpu(4)
        job.storage('20G')

        job.declare_resource_group(output={'vcf.bgz': '{root}.vcf.bgz', 'vcf.bgz.tbi': '{root}.vcf.bgz.tbi'})

        # the echtvar image doesn't have a tool to index, so first add that
        # then run the csq command:
        # -g is the GFF3 file
        # -B 10 is where to truncate long protein changes
        # --local-csq indicates we want each variant annotated independently (haplotype unaware)
        job.command(
            f"""
            bcftools index -t {gnomad_annotated_vcf}
            bcftools csq --force -f {fasta} \
                --local-csq \
                --threads 4 \
                -g {gff3_file} \
                -B 10 \
                -Oz -o {job.output["vcf.bgz"]} \
                --write-index=tbi \
                {gnomad_annotated_vcf}
            """,
        )

        get_batch().write_output(job.output, str(output).removesuffix('.vcf.bgz'))

        return self.make_outputs(dataset, data=output, jobs=job)


@stage(required_stages=AnnotateConsequenceWithBcftools)
class ProcessAnnotatedSitesOnlyVcfIntoHt(DatasetStage):
    """
    Join the annotated sites-only VCF, with AlphaMissense, and with gene/transcript information
    exporting as a HailTable
    """

    def expected_outputs(self, dataset: Dataset) -> Path:
        # output will be a tarball, containing the {dataset.name}_annotations.ht directory
        return self.tmp_prefix / f'{dataset.name}_annotations.ht'

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:

        output = self.expected_outputs(dataset)

        if does_final_file_path_exist(dataset):
            get_logger().info(f'Skipping {self.name} for {dataset.name}, final workflow output already exists')
            return self.make_outputs(dataset, output, jobs=[])

        # pull the alphamissense TarBall location from config, and localise it
        alphamissense = reference_path('alphamissense/ht')

        # mane version for gene details
        mane_version = config_retrieve(['workflow', 'mane_version'], '1.4')
        mane_json = get_batch().read_input(reference_path(f'mane_{mane_version}/json'))

        # ensembl version used to generate region of interest
        ensembl_version = config_retrieve(['workflow', 'ensembl_version'], 113)
        # local file parsed into a dict
        gene_roi = get_batch().read_input(reference_path(f'ensembl_{ensembl_version}/bed'))

        # get the annotated VCF & index
        vcf = str(inputs.as_path(dataset, AnnotateConsequenceWithBcftools))

        job = get_batch().new_job(f'ProcessAnnotatedSitesOnlyVcfIntoHt: {dataset.name}')
        job.image(config_retrieve(['workflow', 'driver_image']))
        job.cpu(4)
        job.storage('20Gi')
        job.memory('highmem')
        job.command(
            f'convert_annotated_vcf_to_ht '
            f'--input {vcf} '
            f'--output {str(output)} '
            f'--gene_bed {gene_roi} '
            f'--am {alphamissense} '
            f'--mane {mane_json} '
            f'--checkpoint_dir {str(self.tmp_prefix / f"{dataset.name}_annotation_checkpoint")} ',
        )

        return self.make_outputs(dataset, data=output, jobs=job)


@stage(required_stages=[ProcessAnnotatedSitesOnlyVcfIntoHt, ExtractDataFromDatasetMt])
class JumpAnnotationsFromHtToFinalMt(DatasetStage):
    """
    Join the annotated sites-only VCF, with AlphaMissense, and with gene/transcript information
    exporting as a HailTable
    """

    def expected_outputs(self, dataset: Dataset) -> Path:
        return self.tmp_prefix / f'{dataset.name}.mt'

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:

        output = self.expected_outputs(dataset)

        if does_final_file_path_exist(dataset):
            get_logger().info(f'Skipping {self.name} for {dataset.name}, final workflow output already exists')
            return self.make_outputs(dataset, output, jobs=[])

        # get the region-limited MT
        mt = str(inputs.as_path(dataset, ExtractDataFromDatasetMt, 'mt'))

        # get the table of compressed annotations
        annotations = str(inputs.as_path(dataset, ProcessAnnotatedSitesOnlyVcfIntoHt))

        job = get_batch().new_job(f'JumpAnnotationsFromHtToFinalMt: {dataset.name}')
        job.image(config_retrieve(['workflow', 'driver_image']))
        job.cpu(16)
        job.memory('highmem')
        job.storage('250Gi')
        job.command(
            f'transfer_annotations_to_vcf '
            f'--input_path {mt} '
            f'--annotations {annotations} '
            f'--output {str(output)}',
        )

        return self.make_outputs(dataset, data=output, jobs=job)


@stage(
    analysis_type='talos_prep',
    required_stages=[JumpAnnotationsFromHtToFinalMt],
)
class SquashMtIntoTarball(DatasetStage):
    """
    Localise the MatrixTable, and create a tarball
    Don't attempt additional compression - it's
    Means that Talos downstream of this won't need GCloud installed
    """

    def expected_outputs(self, dataset: Dataset) -> Path:
        return self.prefix / f'{dataset.name}.mt.tar'

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:

        output = self.expected_outputs(dataset)

        job = get_batch().new_job(f'CompressMtIntoTarball: {dataset.name}')
        job.image(config_retrieve(['workflow', 'driver_image']))
        job.memory('highmem')
        job.cpu(4)
        job.storage(config_retrieve(['talos_prep', 'mt_storage'], '250Gi'))

        # get the annotated MatrixTable - needs to be localised by copy, Hail Batch's service backend can't write local
        mt = str(inputs.as_path(dataset, JumpAnnotationsFromHtToFinalMt))

        # copy the MT into the image, bundle it into a Tar-Ball
        job.command(f'gcloud --no-user-output-enabled storage cp --do-not-decompress -r {mt} $BATCH_TMPDIR')

        # once the data is copied - cd into the tmpdir, then tar it up
        job.command('cd $BATCH_TMPDIR')
        # no compression - the Hail objects are all internally gzipped so not much to gain there
        job.command(f'tar --remove-files -cf {job.output} {dataset.name}.mt')

        get_batch().write_output(job.output, str(output))

        return self.make_outputs(dataset, data=output, jobs=job)
