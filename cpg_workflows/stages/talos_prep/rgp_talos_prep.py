"""
This workflow is designed to generate a minimally annotated dataset for use in Talos

This is a side branch of that, used to specifically process some RGP data which doesn't exactly exist in any expected format

* Step 1: Read a MatrixTable generated by AnnotateDataset
    - Strip out all the prior annotation content
    - Apply a region-filtered based on a provided BED file (only genic regions currently relevant to Talos)
    - Write this minimised MatrixTable to temporary storage
    - Write a sites-only VCF representation in fragments (WAY faster than a serial export)
    - Data is written into tmp

* Step 2: Reassemble the sites-only VCF fragments into a single VCF
    - This uses the gcloud compose operation, which is a rolling merge of the fragments
    - Code re-used from the rd_combiner workflow
    - Data is written into permanent storage, it's only small

* Step 3: Annotate the sites-only VCF with gnomad frequencies
    - This uses Echtvar (https://github.com/populationgenomics/images/tree/main/images/echtvar)
    - Applies a pre-extracted subset of echtvar annotations to the VCF
    - Data is written into tmp

* Step 4: Annotate the gnomAD-annotated VCF with per-transcript consequences
    - Uses bcftools csq, with a GFF3 file and reference genome
    - Data is written into tmp

* Step 5: Process the annotated VCF into a HailTable
    - Loads up the annotated VCF into a Hail Table
    - Splits up the chunky BCSQ annotations into a Hail Array of Hail Structs
    - Adds in per-transcript annotation from MANE and AlphaMissense
    - Saves the final object as a HailTable
    - Data is written into tmp

* Step 6: Load the HailTable into the final MatrixTable
    - Reads the minimised MatrixTable and the HailTable of annotations

* Step 7: Compress the final MatrixTable into a tarball
    - Localise the MatrixTable inside the container using gcloud
    - Compress the MatrixTable into a tarball using zstd
    - Data is written into permanent storage, and registered into Metamist
"""

from cpg_utils import Path
from cpg_utils.config import config_retrieve, image_path, reference_path
from cpg_workflows.targets import Dataset
from cpg_workflows.workflow import DatasetStage, StageInput, StageOutput, get_batch, stage


ORDERED_ALLELES: list[str] = [f'chr{x}' for x in list(range(1, 23))] + ['chrX', 'chrY', 'chrM']


@stage
class SplitVcf(DatasetStage):
    """
    breaks up the whole VCF by chromosome
    """

    def expected_outputs(self, dataset: Dataset) -> dict[str, Path]:
        return {chrom: self.prefix / f'{dataset.name}_split_{chrom}.vcf.gz' for chrom in ORDERED_ALLELES}

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:
        """
        trigger a rolling merge using gcloud compose, gluing all the individual files together
        """

        output = self.expected_outputs(dataset)

        input_vcf = config_retrieve(['workflow', 'input_vcf'])
        local_input = get_batch().read_input_group(
            **{
                'vcf.gz': input_vcf,
                'vcf.gz.tbi': f'{input_vcf}.tbi',
            },
        )

        jobs = []
        for chrom in ORDERED_ALLELES:
            job = get_batch().new_job(f'SplitVcf: {dataset.name} {chrom}')
            job.storage('100GB')
            job.memory('highmem')
            job.cpu(4)
            job.image(image_path('bcftools_120'))

            job.declare_resource_group(
                output={'vcf.gz': '{root}.vcf.gz', 'vcf.gz.tbi': '{root}.vcf.gz.tbi'},
            )

            job.command(
                f"""
                bcftools view \
                --write-index=tbi \
                -Oz -o {job.output["vcf.gz"]} \
                {local_input['vcf.gz']} {chrom}
                """
            )

            get_batch().write_output(job.output, str(output[chrom]).removesuffix('.vcf.gz'))

            jobs.append(job)

        return self.make_outputs(dataset, data=output, jobs=jobs)


@stage(required_stages=[SplitVcf])
class CleanUpVcf(DatasetStage):
    """
    rub subsetting and normalisation on the input VCF
    """

    def expected_outputs(self, dataset: Dataset) -> dict[str, Path]:
        return {chrom: self.prefix / f'{dataset.name}_cleaned_{chrom}.vcf.gz' for chrom in ORDERED_ALLELES}

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:
        """
        trigger a rolling merge using gcloud compose, gluing all the individual files together
        """

        output = self.expected_outputs(dataset)

        bed = reference_path(f'ensembl_113/merged_bed')
        local_bed = get_batch().read_input(bed)

        input_dict = inputs.as_dict(dataset, SplitVcf)

        jobs = []

        for chrom in ORDERED_ALLELES:
            local_input = get_batch().read_input_group(
                **{
                    'vcf.gz': input_dict[chrom],
                    'vcf.gz.tbi': f'{input_dict[chrom]}.tbi',
                },
            )

            job = get_batch().new_job(f'CleanUpVcf: {dataset.name} {chrom}')
            job.storage('100GB')
            job.memory('highmem')
            job.cpu(4)
            job.image(image_path('bcftools_120'))

            job.declare_resource_group(
                output={'vcf.gz': '{root}.vcf.gz', 'vcf.gz.tbi': '{root}.vcf.gz.tbi'},
            )

            job.command(
                f"""
                bcftools norm \
                -m -any \
                --write-index=tbi \
                -R {local_bed} \
                -Oz -o {job.output["vcf.gz"]} \
                {local_input['vcf.gz']}"""
            )

            get_batch().write_output(job.output, str(output[chrom]).removesuffix('.vcf.gz'))

            jobs.append(job)

        return self.make_outputs(dataset, data=output, jobs=jobs)


@stage(required_stages=CleanUpVcf)
class MakeSitesOnlyVersion(DatasetStage):

    def expected_outputs(self, dataset: Dataset) -> dict[str, Path]:

        return {chrom: self.prefix / f'{dataset.name}_cleaned_sitesonly_{chrom}' for chrom in ORDERED_ALLELES}

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:

        output = self.expected_outputs(dataset)

        input_dict = inputs.as_dict(dataset, CleanUpVcf)

        jobs = []

        for chrom in ORDERED_ALLELES:

            input_vcf = get_batch().read_input(input_dict[chrom])

            job = get_batch().new_job(f'MakeSitesOnlyVersion: {chrom}')
            job.image(image_path('bcftools_120'))
            job.storage('50Gi')
            job.memory('highmem')
            job.cpu(4)
            job.command(f'bcftools view -Oz -o {job.output} -G {input_vcf}')

            # TODO swap these again
            get_batch().write_output(job.output, str(output[chrom]).removesuffix('.vcf.gz'))
            # get_batch().write_output(job.output, str(output[chrom]))

            jobs.append(job)

        return self.make_outputs(dataset, data=output, jobs=jobs)


@stage(required_stages=MakeSitesOnlyVersion)
class AnnotateGnomadFrequenciesWithEchtvar(DatasetStage):
    """
    Annotate this cohort joint-call VCF with gnomad frequencies, write to tmp storage
    """

    def expected_outputs(self, dataset: Dataset) -> dict[str, Path]:

        return {chrom: self.prefix / f'{dataset.name}_gnomad_{chrom}.vcf.gz' for chrom in ORDERED_ALLELES}

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:
        output = self.expected_outputs(dataset)

        # this is a single whole-genome file, generated by the echtvar workflow
        gnomad_annotations = get_batch().read_input(config_retrieve(['annotations', 'echtvar_gnomad']))

        input_dict = inputs.as_dict(dataset, MakeSitesOnlyVersion)

        jobs = []

        for chrom in ORDERED_ALLELES:

            # todo scrub this rmsuffix
            sites_vcf = get_batch().read_input(str(input_dict[chrom]).removesuffix('.vcf.gz'))

            job = get_batch().new_job(f'AnnotateGnomadFrequenciesWithEchtvar: {dataset.name} {chrom}')
            job.image(image_path('echtvar'))
            job.command(f'echtvar anno -e {gnomad_annotations} {sites_vcf} {job.output}')
            job.storage('20Gi')
            job.memory('highmem')
            job.cpu(4)

            get_batch().write_output(job.output, str(output[chrom]))

            jobs.append(job)

        return self.make_outputs(dataset, data=output, jobs=jobs)


@stage(required_stages=AnnotateGnomadFrequenciesWithEchtvar)
class AnnotateConsequenceWithBcftools(DatasetStage):
    """
    Take the VCF with gnomad frequencies, and annotate with consequences using BCFtools
    Writes into a cohort-specific permanent folder
    """

    def expected_outputs(self, dataset: Dataset) -> dict[str, Path]:

        return {chrom: self.prefix / f'{dataset.name}_csq_{chrom}.vcf.gz' for chrom in ORDERED_ALLELES}

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:
        output = self.expected_outputs(dataset)

        input_dict = inputs.as_dict(dataset, AnnotateGnomadFrequenciesWithEchtvar)

        jobs = []

        # get the GFF3 file required to generate consequences
        gff3_file = get_batch().read_input(reference_path(f'ensembl_113/gff3'))

        # get the fasta
        fasta = get_batch().read_input(reference_path('broad/ref_fasta'))

        for chrom in ORDERED_ALLELES:
            sites_vcf = get_batch().read_input(input_dict[chrom])

            job = get_batch().new_job(f'AnnotateConsequenceWithBcftools: {dataset.name} {chrom}')

            job.declare_resource_group(output={'vcf.gz': '{root}.vcf.gz', 'vcf.gz.tbi': '{root}.vcf.gz.tbi'})

            job.image(image_path('bcftools_120'))
            job.command(f'bcftools index -t {sites_vcf}')
            job.command(
                f"""
                bcftools csq --force -f {fasta} \
                --local-csq \
                --threads 4 \
                -g {gff3_file} \
                -B 10 \
                -Oz -o {job.output["vcf.gz"]} \
                --write-index=tbi \
                {sites_vcf}
                """
            )
            job.storage('20Gi')
            job.memory('highmem')
            job.cpu(4)

            get_batch().write_output(job.output, str(output[chrom]).removesuffix('.vcf.gz'))

            jobs.append(job)

        return self.make_outputs(dataset, data=output, jobs=jobs)


@stage(required_stages=AnnotateConsequenceWithBcftools)
class ProcessAnnotatedSitesOnlyVcfIntoHt(DatasetStage):
    """
    Join the annotated sites-only VCF, with AlphaMissense, and with gene/transcript information
    exporting as a HailTable
    """

    def expected_outputs(self, dataset: Dataset) -> Path:
        # output will be a tarball, containing the {dataset.name}_annotations.ht directory
        return self.tmp_prefix / f'{dataset.name}_annotations.ht'

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:

        output = self.expected_outputs(dataset)

        input_dict = inputs.as_dict(dataset, AnnotateConsequenceWithBcftools)

        # pull the alphamissense TarBall location from config, and localise it
        alphamissense = reference_path('alphamissense/ht')

        # mane version for gene details
        mane_json = get_batch().read_input(reference_path(f'mane_1.4/json'))

        # ensembl version used to generate region of interest
        ensembl_version = config_retrieve(['workflow', 'ensembl_version'], 113)

        # local file parsed into a dict
        gene_roi = get_batch().read_input(reference_path(f'ensembl_{ensembl_version}/bed'))

        all_ordered_vcfs = " ".join(str(input_dict[chrom]) for chrom in ORDERED_ALLELES)

        job = get_batch().new_job(f'ProcessAnnotatedSitesOnlyVcfIntoHt: {dataset.name}')
        job.image(config_retrieve(['workflow', 'driver_image']))
        job.cpu(4)
        job.storage('20Gi')
        job.memory('highmem')
        job.command(
            'convert_annotated_vcf_to_ht '
            f'--input {all_ordered_vcfs} '
            f'--output {str(output)} '
            f'--gene_bed {gene_roi} '
            f'--am {alphamissense} '
            f'--mane {mane_json} '
            f'--checkpoint_dir {str(self.tmp_prefix / f"{dataset.name}_annotation_checkpoint")} ',
        )

        return self.make_outputs(dataset, data=output, jobs=job)


@stage(required_stages=[ProcessAnnotatedSitesOnlyVcfIntoHt, CleanUpVcf])
class TransferAnnotationsToMt(DatasetStage):
    """
    Join the annotated sites-only VCF, with AlphaMissense, and with gene/transcript information
    exporting as a HailTable
    """

    def expected_outputs(self, dataset: Dataset) -> Path:
        return self.tmp_prefix / f'{dataset.name}.mt'

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:

        output = self.expected_outputs(dataset)

        # get the region-limited MT
        input_dict = inputs.as_dict(dataset, CleanUpVcf)
        all_ordered_vcfs = " ".join(str(input_dict[chrom]) for chrom in ORDERED_ALLELES)

        # get the table of compressed annotations
        annotations = str(inputs.as_path(dataset, ProcessAnnotatedSitesOnlyVcfIntoHt))

        job = get_batch().new_job(f'JumpAnnotationsFromHtToFinalMt: {dataset.name}')
        job.image(config_retrieve(['workflow', 'driver_image']))
        job.cpu(16)
        job.memory('highmem')
        job.storage('250Gi')
        job.command(
            f'TransferAnnotationsToMatrixTable '
            f'--input {all_ordered_vcfs} '
            f'--annotations {annotations} '
            f'--output {str(output)}',
        )

        return self.make_outputs(dataset, data=output, jobs=job)


@stage(
    analysis_type='talos_prep',
    required_stages=[TransferAnnotationsToMt],
)
class SquashMtIntoTarball_rgp(DatasetStage):
    """
    Localise the MatrixTable, and create a tarball
    Don't attempt additional compression - it's
    Means that Talos downstream of this won't need GCloud installed
    """

    def expected_outputs(self, dataset: Dataset) -> Path:
        return self.prefix / f'{dataset.name}.mt.tar'

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:

        output = self.expected_outputs(dataset)

        job = get_batch().new_job(f'CompressMtIntoTarball: {dataset.name}')
        job.image(config_retrieve(['workflow', 'driver_image']))
        job.memory('highmem')
        job.cpu(4)
        job.storage(config_retrieve(['talos_prep', 'mt_storage'], '250Gi'))

        # get the annotated MatrixTable - needs to be localised by copy, Hail Batch's service backend can't write local
        mt = str(inputs.as_path(dataset, TransferAnnotationsToMt))

        # copy the MT into the image, bundle it into a Tar-Ball
        job.command(f'gcloud --no-user-output-enabled storage cp --do-not-decompress -r {mt} $BATCH_TMPDIR')

        # once the data is copied - cd into the tmpdir, then tar it up
        job.command('cd $BATCH_TMPDIR')
        # no compression - the Hail objects are all internally gzipped so not much to gain there
        job.command(f'tar --remove-files -cf {job.output} {dataset.name}.mt')

        get_batch().write_output(job.output, str(output))

        return self.make_outputs(dataset, data=output, jobs=job)
