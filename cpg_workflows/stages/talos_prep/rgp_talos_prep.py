"""
This workflow is designed to generate a minimally annotated dataset for use in Talos

This is a side branch of that, used to specifically process some RGP data which doesn't exactly exist in any expected format

* Step 1: Read a MatrixTable generated by AnnotateDataset
    - Strip out all the prior annotation content
    - Apply a region-filtered based on a provided BED file (only genic regions currently relevant to Talos)
    - Write this minimised MatrixTable to temporary storage
    - Write a sites-only VCF representation in fragments (WAY faster than a serial export)
    - Data is written into tmp

* Step 2: Reassemble the sites-only VCF fragments into a single VCF
    - This uses the gcloud compose operation, which is a rolling merge of the fragments
    - Code re-used from the rd_combiner workflow
    - Data is written into permanent storage, it's only small

* Step 3: Annotate the sites-only VCF with gnomad frequencies
    - This uses Echtvar (https://github.com/populationgenomics/images/tree/main/images/echtvar)
    - Applies a pre-extracted subset of echtvar annotations to the VCF
    - Data is written into tmp

* Step 4: Annotate the gnomAD-annotated VCF with per-transcript consequences
    - Uses bcftools csq, with a GFF3 file and reference genome
    - Data is written into tmp

* Step 5: Process the annotated VCF into a HailTable
    - Loads up the annotated VCF into a Hail Table
    - Splits up the chunky BCSQ annotations into a Hail Array of Hail Structs
    - Adds in per-transcript annotation from MANE and AlphaMissense
    - Saves the final object as a HailTable
    - Data is written into tmp

* Step 6: Load the HailTable into the final MatrixTable
    - Reads the minimised MatrixTable and the HailTable of annotations

* Step 7: Compress the final MatrixTable into a tarball
    - Localise the MatrixTable inside the container using gcloud
    - Compress the MatrixTable into a tarball using zstd
    - Data is written into permanent storage, and registered into Metamist
"""

from cpg_utils import Path
from cpg_utils.config import config_retrieve, image_path, reference_path
from cpg_workflows.targets import Dataset
from cpg_workflows.workflow import DatasetStage, StageInput, StageOutput, get_batch, stage

from cpg_workflows.scripts.talos_prep import convert_annotated_vcf_to_ht, TransferAnnotationsToMatrixTable


@stage
class CleanUpVcf(DatasetStage):
    """
    rub subsetting and normalisation on the input VCF
    """

    def expected_outputs(self, dataset: Dataset) -> Path:
        return self.prefix / f'{dataset.name}_cleaned.vcf.bgz'

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:
        """
        trigger a rolling merge using gcloud compose, gluing all the individual files together
        """

        output = self.expected_outputs(dataset)

        input_vcf = config_retrieve(['workflow', 'input_vcf'])
        local_input = get_batch().read_input(input_vcf)

        bed = reference_path(f'ensembl_113/merged_bed')
        local_bed = get_batch().read_input(bed)

        job = get_batch().new_job(f'CleanUpRgpVcf')
        job.storage('150GB')
        job.memory('highmem')
        job.image(image_path('bcftools_120'))

        job.declare_resource_group(
            output={'vcf.gz': '{root}.vcf.gz', 'vcf.gz.tbi': '{root}.vcf.gz.tbi'},
        )

        job.command(
            f"""
            bcftools norm \
            -m -any \
            --write-index=tbi \
            -R {local_bed} \
            -Oz -o {job.output["vcf.gz"]} \
            {local_input}"""
        )

        get_batch().write_output(job.output, str(output).removesuffix('.vcf.bgz'))

        return self.make_outputs(dataset, data=output, jobs=job)


@stage(required_stages=CleanUpVcf)
class MakeSitesOnlyVersion(DatasetStage):

    def expected_outputs(self, dataset: Dataset) -> Path:
        return self.prefix / f'{dataset.name}_cleaned_sitesonly.vcf.bgz'

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:

        output = self.expected_outputs(dataset)

        input_vcf = inputs.as_path(dataset, CleanUpVcf)

        job = get_batch().new_job(f'MakeSitesOnlyVersion')
        job.image(image_path('bcftools_120'))
        job.storage('50Gi')
        job.memory('highmem')
        job.command(f'bcftools view -Oz -o {job.output} -G {input_vcf}')

        get_batch().write_output(job.output, str(output).removesuffix('.vcf.bgz'))

        return self.make_outputs(dataset, data=output, jobs=job)


@stage(required_stages=MakeSitesOnlyVersion)
class AnnotateGnomadFrequenciesWithEchtvar(DatasetStage):
    """
    Annotate this cohort joint-call VCF with gnomad frequencies, write to tmp storage
    """

    def expected_outputs(self, dataset: Dataset) -> Path:
        return self.tmp_prefix / f'{dataset.name}_gnomad_frequency_annotated.vcf.bgz'

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:
        outputs = self.expected_outputs(dataset)

        sites_vcf = get_batch().read_input(inputs.as_path(dataset, MakeSitesOnlyVersion))

        # this is a single whole-genome file, generated by the echtvar workflow
        gnomad_annotations = get_batch().read_input(config_retrieve(['annotations', 'echtvar_gnomad']))

        job = get_batch().new_job(f'AnnotateGnomadFrequenciesWithEchtvar: {dataset.name}')
        job.image(image_path('echtvar'))
        job.command(f'echtvar anno -e {gnomad_annotations} {sites_vcf} {job.output}')
        job.storage('20Gi')
        job.memory('highmem')
        job.cpu(4)

        get_batch().write_output(job.output, str(outputs))

        return self.make_outputs(dataset, data=outputs, jobs=job)


@stage(required_stages=AnnotateGnomadFrequenciesWithEchtvar)
class AnnotateConsequenceWithBcftools(DatasetStage):
    """
    Take the VCF with gnomad frequencies, and annotate with consequences using BCFtools
    Writes into a cohort-specific permanent folder
    """

    def expected_outputs(self, dataset: Dataset) -> Path:
        return self.tmp_prefix / f'{dataset.name}_consequence_annotated.vcf.bgz'

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:
        output = self.expected_outputs(dataset)

        gnomad_annotated_vcf = get_batch().read_input(
            inputs.as_path(dataset, AnnotateGnomadFrequenciesWithEchtvar),
        )

        # get the GFF3 file required to generate consequences
        gff3_file = get_batch().read_input(reference_path(f'ensembl_113/gff3'))

        # get the fasta
        fasta = get_batch().read_input(reference_path('broad/ref_fasta'))

        job = get_batch().new_job(f'AnnotateConsequenceWithBcftools: {dataset.name}')
        job.image(image_path('bcftools_120'))
        job.cpu(4)
        job.storage('20G')

        job.declare_resource_group(output={'vcf.bgz': '{root}.vcf.bgz', 'vcf.bgz.tbi': '{root}.vcf.bgz.tbi'})

        # the echtvar image doesn't have a tool to index, so first add that
        # then run the csq command:
        # -g is the GFF3 file
        # -B 10 is where to truncate long protein changes
        # --local-csq indicates we want each variant annotated independently (haplotype unaware)
        job.command(
            f"""
            bcftools index -t {gnomad_annotated_vcf}
            bcftools csq --force -f {fasta} \
                --local-csq \
                --threads 4 \
                -g {gff3_file} \
                -B 10 \
                -Oz -o {job.output["vcf.bgz"]} \
                --write-index=tbi \
                {gnomad_annotated_vcf}
            """,
        )

        get_batch().write_output(job.output, str(output).removesuffix('.vcf.bgz'))

        return self.make_outputs(dataset, data=output, jobs=job)


@stage(required_stages=AnnotateConsequenceWithBcftools)
class ProcessAnnotatedSitesOnlyVcfIntoHt(DatasetStage):
    """
    Join the annotated sites-only VCF, with AlphaMissense, and with gene/transcript information
    exporting as a HailTable
    """

    def expected_outputs(self, dataset: Dataset) -> Path:
        # output will be a tarball, containing the {dataset.name}_annotations.ht directory
        return self.tmp_prefix / f'{dataset.name}_annotations.ht'

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:

        output = self.expected_outputs(dataset)

        # pull the alphamissense TarBall location from config, and localise it
        alphamissense = reference_path('alphamissense/ht')

        # mane version for gene details
        mane_json = get_batch().read_input(reference_path(f'mane_1.4/json'))

        # ensembl version used to generate region of interest
        ensembl_version = config_retrieve(['workflow', 'ensembl_version'], 113)
        # local file parsed into a dict
        gene_roi = get_batch().read_input(reference_path(f'ensembl_{ensembl_version}/bed'))

        # get the annotated VCF & index
        vcf = str(inputs.as_path(dataset, AnnotateConsequenceWithBcftools))

        job = get_batch().new_job(f'ProcessAnnotatedSitesOnlyVcfIntoHt: {dataset.name}')
        job.image(config_retrieve(['workflow', 'driver_image']))
        job.cpu(4)
        job.storage('20Gi')
        job.memory('highmem')
        job.command(
            f'{convert_annotated_vcf_to_ht.__file__} '
            f'--input {vcf} '
            f'--output {str(output)} '
            f'--gene_bed {gene_roi} '
            f'--am {alphamissense} '
            f'--mane {mane_json} '
            f'--checkpoint_dir {str(self.tmp_prefix / f"{dataset.name}_annotation_checkpoint")} ',
        )

        return self.make_outputs(dataset, data=output, jobs=job)


@stage(required_stages=[ProcessAnnotatedSitesOnlyVcfIntoHt, CleanUpVcf])
class TransferAnnotationsToMt(DatasetStage):
    """
    Join the annotated sites-only VCF, with AlphaMissense, and with gene/transcript information
    exporting as a HailTable
    """

    def expected_outputs(self, dataset: Dataset) -> Path:
        return self.tmp_prefix / f'{dataset.name}.mt'

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:

        output = self.expected_outputs(dataset)

        # get the region-limited MT
        vcf = str(inputs.as_path(dataset, CleanUpVcf))

        # get the table of compressed annotations
        annotations = str(inputs.as_path(dataset, ProcessAnnotatedSitesOnlyVcfIntoHt))

        job = get_batch().new_job(f'JumpAnnotationsFromHtToFinalMt: {dataset.name}')
        job.image(config_retrieve(['workflow', 'driver_image']))
        job.cpu(16)
        job.memory('highmem')
        job.storage('250Gi')
        job.command(
            f'{TransferAnnotationsToMatrixTable.__file__} '
            f'--input_path {vcf} '
            f'--annotations {annotations} '
            f'--output {str(output)}',
        )

        return self.make_outputs(dataset, data=output, jobs=job)


@stage(
    analysis_type='talos_prep',
    required_stages=[TransferAnnotationsToMt],
)
class SquashMtIntoTarball_rgp(DatasetStage):
    """
    Localise the MatrixTable, and create a tarball
    Don't attempt additional compression - it's
    Means that Talos downstream of this won't need GCloud installed
    """

    def expected_outputs(self, dataset: Dataset) -> Path:
        return self.prefix / f'{dataset.name}.mt.tar'

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:

        output = self.expected_outputs(dataset)

        job = get_batch().new_job(f'CompressMtIntoTarball: {dataset.name}')
        job.image(config_retrieve(['workflow', 'driver_image']))
        job.memory('highmem')
        job.cpu(4)
        job.storage(config_retrieve(['talos_prep', 'mt_storage'], '250Gi'))

        # get the annotated MatrixTable - needs to be localised by copy, Hail Batch's service backend can't write local
        mt = str(inputs.as_path(dataset, TransferAnnotationsToMt))

        # copy the MT into the image, bundle it into a Tar-Ball
        job.command(f'gcloud --no-user-output-enabled storage cp --do-not-decompress -r {mt} $BATCH_TMPDIR')

        # once the data is copied - cd into the tmpdir, then tar it up
        job.command('cd $BATCH_TMPDIR')
        # no compression - the Hail objects are all internally gzipped so not much to gain there
        job.command(f'tar --remove-files -cf {job.output} {dataset.name}.mt')

        get_batch().write_output(job.output, str(output))

        return self.make_outputs(dataset, data=output, jobs=job)
