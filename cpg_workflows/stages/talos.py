"""
This is a central script for the stages associated with Talos (previously AIP)

Due to privacy concerns, any configuration elements specific to an individual
project are stored in a separate config file, which is in a private repository.
 - production-pipelines-configuration/blob/main/config/talos/talos.toml

The cohort/project specific elements are described at the bottom of the Talos default config file here:
 - production-pipelines/blob/main/configs/default/talos.toml

[cohorts.DATASET_NAME]
- cohort_panels, list[int], PanelApp panel IDs to apply to this project. By default, only the Mendeliome (137) is
  applied to all analyses.

[cohorts.DATASET_NAME.genome]  # repeated for exome if appropriate
- DATASET_NAME is taken from config[workflow][dataset]
- historic_results, str, path to historic results directory for this project. If present, this folder will be used to
  detect the results from the latest run (so that variants which have been previously seen are annotated with the date
  they were first seen), and at the conclusion of the analysis a new version of the file is written with an additional
  new results from this run.
  If this config entry is absent, no previous results will be identified, so all results will have a 'first_seen'
  annotation of today's date, and the results from the current run won't be able to inform the next run.
- seqr_instance, str, URL for this seqr instance. Remove if not in seqr
- seqr_project, str, project ID for this project/seq type. Remove if not in seqr
- seqr_lookup, str, path to JSON file containing the mapping of CPG internal IDs to Seqr IDs, as generated by
  talos/helpers/process_seqr_metadata.py. If a new Seqr load has been performed, this file will need to be updated. This
  is stored in the test bucket so that it can be overwritten by a user. Remove if cohort is not in Seqr.

Takes as input:
    - Annotated MT path (either found in metamist or directly in config)
    - HPO.obo file (read from cpg-common reference location)
    - Seqr<->Internal ID mapping file (if appropriate, from Config)
    - History folder (if appropriate, from Config)
Generates:
    - Config file for this run
    - PED file for this cohort (extended format, with additional columns for Ext. ID and HPO terms)
    - Latest panels found through HPO matches against this cohort
    - PanelApp results
    - A mapping of all the Gene IDs in PanelApp to their Gene Symbol
    - Category-labelled VCF
    - Talos results JSON (metamist `aip-results` analysis)
    - Talos results + phenotype-match annotation
    - Talos report HTML (metamist `aip-report` analysis)

This will have to be run using Full permissions as we will need to reference data in test and main buckets.
"""

from datetime import datetime
from functools import cache, lru_cache
from os.path import join
from random import randint

import toml

from cpg_utils import Path, to_path
from cpg_utils.config import ConfigError, config_retrieve, image_path
from cpg_utils.hail_batch import authenticate_cloud_credentials_in_job, get_batch
from cpg_workflows.resources import STANDARD
from cpg_workflows.targets import Dataset
from cpg_workflows.utils import exists, get_logger, tshirt_mt_sizing
from cpg_workflows.workflow import DatasetStage, StageInput, StageOutput, stage
from metamist.graphql import gql, query

CHUNKY_DATE = datetime.now().strftime('%Y-%m-%d')
MTA_QUERY = gql(
    """
    query MyQuery($dataset: String!, $type: String!) {
        project(name: $dataset) {
            analyses(active: {eq: true}, type: {eq: $type}, status: {eq: COMPLETED}) {
                output
                timestampCompleted
                meta
            }
        }
    }
""",
)
# used when building a runtime configuration
SEQR_KEYS: list[str] = ['seqr_project', 'seqr_instance', 'seqr_lookup']
TALOS_PREP_TYPE = 'talos_prep'


@lru_cache(maxsize=1)
def get_date_string() -> str:
    """
    allows override of the date folder to continue/re-run previous analyses

    Returns:
        either an override in config, or the default (today, YYYY-MM-DD)
    """
    return config_retrieve(['workflow', 'date_folder_override'], CHUNKY_DATE)


@lru_cache(1)
def get_date_folder() -> str:
    """
    allows override of the date folder to continue/re-run previous analyses
    Returns:
        either an override in config, or the default "reanalysis/(today, YYYY-MM-DD)"
    """
    return join('reanalysis', get_date_string())


@cache
def query_for_sv_vcf(dataset: str) -> str | None:
    """
    query for the latest SV VCF for a dataset
    return a tuple of full path and filename, or None if not found

    Args:
        dataset (str): project to query for

    Returns:
        str, the path to the latest VCF for the given type
    """
    sequencing_type = config_retrieve(['workflow', 'sequencing_type'])
    analysis_type = 'single_dataset_cnv_annotated' if sequencing_type == 'exome' else 'single_dataset_sv_annotated'

    # hot swapping to a string we can freely modify
    query_dataset = dataset

    if config_retrieve(['workflow', 'access_level']) == 'test' and 'test' not in query_dataset:
        query_dataset += '-test'

    result = query(MTA_QUERY, variables={'dataset': query_dataset, 'type': analysis_type})
    vcf_by_date: dict[str, str] = {}
    for analysis in result['project']['analyses']:
        vcf_by_date[analysis['timestampCompleted']] = analysis['output']

    # perfectly acceptable to not have an input SV MT
    if not vcf_by_date:
        return None

    # return the latest, determined by a sort on timestamp
    # 2023-10-10... > 2023-10-09..., so sort on strings
    return vcf_by_date[sorted(vcf_by_date)[-1]]


@lru_cache(maxsize=None)
def query_for_latest_hail_object(
    dataset: str,
    analysis_type: str,
    object_suffix: str = '.mt',
    exact_string: str | None = None,
) -> str:
    """
    query for the latest MT for a dataset
    the exact metamist entry type to search for is handled by config, defaulting to the new rd_combiner MT
    Args:
        dataset (str):       project to query for
        analysis_type (str): analysis type to query for - rd_combiner writes MTs to metamist as 'matrixtable',
                             seqr_loader used 'custom': using a config entry we can decide which type to use
        object_suffix (str): suffix to detect on analysis path (choose between .mt or .ht)
        exact_string (str):  if set, analysis entries will only be considered if this substring is in the path
                             the intended use here is targeting outputs from a single Stage (annotate cohort vs dataset)
    Returns:
        str, the path to the latest object for the given type
    """

    # hot swapping to a string we can freely modify
    query_dataset = dataset
    if config_retrieve(['workflow', 'access_level']) == 'test' and 'test' not in query_dataset:
        query_dataset += '-test'

    get_logger().info(f'Querying for {analysis_type} in {query_dataset}')

    result = query(MTA_QUERY, variables={'dataset': query_dataset, 'type': analysis_type})

    # get all the relevant entries, and bin by date
    mt_by_date = {}
    for analysis in result['project']['analyses']:
        if (
            analysis['output']
            and analysis['output'].endswith(object_suffix)
            and (analysis['meta']['sequencing_type'] == config_retrieve(['workflow', 'sequencing_type']))
        ):
            # if exact string is absent, or set and in this output path, use it
            if (exact_string is None) or (exact_string in analysis['output']):
                mt_by_date[analysis['timestampCompleted']] = analysis['output']

    if not mt_by_date:
        raise ValueError(f'No MT found for dataset {query_dataset}')

    # return the latest, determined by a sort on timestamp
    # 2023-10-10... > 2023-10-09..., so sort on strings
    return mt_by_date[sorted(mt_by_date)[-1]]


@lru_cache(2)
def get_clinvar_table(key: str = 'clinvarbitration') -> str:
    """
    this is used to retrieve two types of object - clinvar_decisions & clinvar_pm5
    these are packaged into one tarball

    try and identify the clinvar table to use
    - try the config specified path
    - fall back to storage:common default path - try with multiple dir names in case we change this
    - if neither works, choose to fail instead

    Args
        key (str): the key to look for in the config

    Returns:
        a path to a clinvar table, or None
    """

    if (clinvar_table := config_retrieve(['workflow', key], None)) is not None:
        get_logger().info(f'Using clinvar table {clinvar_table} from config')
        return clinvar_table

    get_logger().info(f'No forced {key} table available, trying default')

    # try multiple path variations - legacy dir name is 'aip_clinvar', but this may also change
    clinvar_table = join(
        config_retrieve(['storage', 'common', 'analysis']),
        'clinvarbitration',
        datetime.now().strftime('%y-%m'),
        'clinvar_decisions.release.tar.gz',
    )

    if to_path(clinvar_table).exists():
        get_logger().info(f'Using clinvar table {clinvar_table}')
        return clinvar_table

    raise ValueError('no Clinvar Tables were identified')


@stage
class GeneratePED(DatasetStage):
    """
    revert to just using the metamist/CPG-flow Pedigree generation
    """

    def expected_outputs(self, dataset: Dataset) -> Path:
        return dataset.prefix() / get_date_folder() / 'pedigree.ped'

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:
        expected_out = self.expected_outputs(dataset)
        pedigree = dataset.write_ped_file(out_path=expected_out)
        get_logger().info(f'PED file for {dataset.name} written to {pedigree}')

        return self.make_outputs(dataset, data=expected_out)


@stage
class MakeRuntimeConfig(DatasetStage):
    """
    create a config file for this run,
    this new config should include all elements specific to this Project and sequencing_type
    this new unambiguous config file should be used in all jobs
    """

    def expected_outputs(self, dataset: Dataset) -> Path:
        return dataset.prefix() / get_date_folder() / 'config.toml'

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:

        # start off with a fresh config dictionary, including generic content
        new_config: dict = {
            'categories': config_retrieve(['categories']),
            'GeneratePanelData': config_retrieve(['GeneratePanelData']),
            'RunHailFiltering': config_retrieve(['RunHailFiltering']),
            'ValidateMOI': config_retrieve(['ValidateMOI']),
            'HPOFlagging': config_retrieve(['HPOFlagging']),
            'CreateTalosHTML': {},
        }

        # pull the content relevant to this cohort + sequencing type (mandatory in CPG)
        seq_type = config_retrieve(['workflow', 'sequencing_type'])
        dataset_conf = config_retrieve(['cohorts', dataset.name])
        seq_type_conf = dataset_conf.get(seq_type, {})

        # forbidden genes and forced panels
        new_config['GeneratePanelData']['forbidden_genes'] = dataset_conf.get('forbidden', [])
        new_config['GeneratePanelData']['forced_panels'] = dataset_conf.get('forced_panels', [])
        new_config['GeneratePanelData']['blacklist'] = dataset_conf.get('blacklist', None)

        # optionally, all SG IDs to remove from analysis
        new_config['ValidateMOI']['solved_cases'] = dataset_conf.get('solved_cases', [])

        # adapt to new hyperlink config structure
        if hyperlinks := seq_type_conf.get('hyperlinks'):
            new_config['CreateTalosHTML']['hyperlinks'] = hyperlinks

        if 'external_labels' in seq_type_conf:
            new_config['CreateTalosHTML']['external_labels'] = seq_type_conf['external_labels']

        # add a location for the run history files
        if 'result_history' in seq_type_conf:
            new_config['result_history'] = seq_type_conf['result_history']

        expected_outputs = self.expected_outputs(dataset)

        with expected_outputs.open('w') as write_handle:
            toml.dump(new_config, write_handle)

        return self.make_outputs(target=dataset, data=expected_outputs)


@stage
class MakePhenopackets(DatasetStage):
    """
    this calls the script which reads phenotype data from metamist
    and generates a phenopacket file (GA4GH compliant)
    """

    def expected_outputs(self, dataset: Dataset) -> Path:
        return dataset.prefix() / get_date_folder() / 'phenopackets.json'

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:
        """
        generate a pedigree from metamist
        script to generate an extended pedigree format - additional columns for Ext. ID and HPO terms
        """
        job = get_batch().new_job('Generate Phenopackets from Metamist')
        job.cpu(1).image(image_path('talos'))

        expected_out = self.expected_outputs(dataset)
        query_dataset = dataset.name
        if config_retrieve(['workflow', 'access_level']) == 'test' and 'test' not in query_dataset:
            query_dataset += '-test'

        hpo_file = get_batch().read_input(config_retrieve(['GeneratePanelData', 'obo_file']))

        # mandatory argument
        seq_type = config_retrieve(['workflow', 'sequencing_type'])

        # insert a little stagger
        job.command(f'sleep {randint(0, 30)}')

        job.command(
            'MakePhenopackets '
            f'--dataset {query_dataset} '
            f'--output {job.output} '
            f'--type {seq_type} '
            f'--hpo {hpo_file}',
        )
        get_batch().write_output(job.output, str(expected_out))
        get_logger().info(f'Phenopacket file for {dataset.name} going to {expected_out}')

        return self.make_outputs(dataset, data=expected_out, jobs=job)


@stage(required_stages=[MakePhenopackets, MakeRuntimeConfig])
class GeneratePanelData(DatasetStage):
    """
    PythonJob to find HPO-matched panels
    """

    def expected_outputs(self, dataset: Dataset) -> Path:
        """
        only one output, the panel data
        """
        return dataset.prefix() / get_date_folder() / 'hpo_panel_data.json'

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:
        job = get_batch().new_job(f'Find HPO-matched Panels: {dataset.name}')
        job.cpu(1).image(image_path('talos'))

        # use the new config file
        runtime_config = str(inputs.as_path(dataset, MakeRuntimeConfig))
        conf_in_batch = get_batch().read_input(runtime_config)

        expected_out = self.expected_outputs(dataset)

        hpo_file = get_batch().read_input(config_retrieve(['GeneratePanelData', 'obo_file']))
        local_phenopacket = get_batch().read_input(str(inputs.as_path(target=dataset, stage=MakePhenopackets)))

        job.command(f'export TALOS_CONFIG={conf_in_batch}')
        # insert a little stagger

        job.command(f'sleep {randint(0, 30)}')
        job.command(f'GeneratePanelData --input {local_phenopacket} --output {job.output} --hpo {hpo_file}')
        get_batch().write_output(job.output, str(expected_out))

        return self.make_outputs(dataset, data=expected_out, jobs=job)


@stage(required_stages=[GeneratePanelData, MakeRuntimeConfig])
class QueryPanelapp(DatasetStage):
    """
    query PanelApp for up-to-date gene lists
    """

    def expected_outputs(self, dataset: Dataset) -> Path:
        return dataset.prefix() / get_date_folder() / 'panelapp_data.json'

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:
        job = get_batch().new_job(f'Query PanelApp: {dataset.name}')
        job.cpu(1).image(image_path('talos'))

        # use the new config file
        runtime_config = str(inputs.as_path(dataset, MakeRuntimeConfig))
        conf_in_batch = get_batch().read_input(runtime_config)

        hpo_panel_json = inputs.as_path(target=dataset, stage=GeneratePanelData)
        expected_out = self.expected_outputs(dataset)
        job.command(f'export TALOS_CONFIG={conf_in_batch}')
        # insert a little stagger
        job.command(f'sleep {randint(20, 300)}')
        job.command(f'QueryPanelapp --input {get_batch().read_input(str(hpo_panel_json))} --output {job.output}')
        get_batch().write_output(job.output, str(expected_out))

        return self.make_outputs(dataset, data=expected_out, jobs=job)


@stage(required_stages=[QueryPanelapp, GeneratePED, MakeRuntimeConfig])
class RunHailFiltering(DatasetStage):
    """
    hail job to filter & label the MT
    """

    def expected_outputs(self, dataset: Dataset) -> Path:
        return dataset.prefix() / get_date_folder() / 'hail_labelled.vcf.bgz'

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:

        # if it's not set in config, try and get it through metamist
        if not (input_mt := config_retrieve(['workflow', 'matrix_table'], None)):

            input_mt = query_for_latest_hail_object(
                dataset=dataset.name,
                analysis_type=TALOS_PREP_TYPE,
                object_suffix='.mt.tar',
            )

        job = get_batch().new_job(f'Run hail labelling: {dataset.name}')
        job.image(image_path('talos'))
        job.command('set -eux pipefail')

        # time in seconds before this jobs self-destructs
        # some recent runs of this have hit the GCP copy and... hung indefinitely at great expense
        # the highest current runtime when successful is just shy of 4 hours
        job.timeout(config_retrieve(['RunHailFiltering', 'timeouts', 'small_variants'], 15000))

        # use the new config file
        runtime_config = str(inputs.as_path(dataset, MakeRuntimeConfig))
        conf_in_batch = get_batch().read_input(runtime_config)

        # MTs can vary from <10GB for a small exome, to 170GB for a larger one, Genomes ~500GB
        required_storage = tshirt_mt_sizing(
            sequencing_type=config_retrieve(['workflow', 'sequencing_type']),
            cohort_size=len(dataset.get_sequencing_group_ids()),
        )
        required_cpu: int = config_retrieve(['RunHailFiltering', 'cores', 'small_variants'], 8)
        required_mem: str = config_retrieve(['RunHailFiltering', 'memory', 'small_variants'], 'highmem')

        # doubling storage due to the repartitioning
        job.cpu(required_cpu).storage(f'{required_storage*2}Gi').memory(required_mem)

        panelapp_json = get_batch().read_input(str(inputs.as_path(target=dataset, stage=QueryPanelapp)))
        pedigree = inputs.as_path(target=dataset, stage=GeneratePED)
        expected_out = self.expected_outputs(dataset)

        # copy vcf & index out manually
        job.declare_resource_group(output={'vcf.bgz': '{root}.vcf.bgz', 'vcf.bgz.tbi': '{root}.vcf.bgz.tbi'})

        # peds can't read cloud paths
        local_ped = get_batch().read_input(str(pedigree))

        # see if we can find any exomiser results to integrate
        try:
            if 'exomiser' in config_retrieve(['ValidateMOI']):
                raise ValueError('Exomiser is not required in this workflow')

            exomiser_tar = query_for_latest_hail_object(
                dataset=dataset.name,
                analysis_type='exomiser',
                object_suffix='.ht.tar',
            )
            localised_exomiser = get_batch().read_input(exomiser_tar)

            job.command(f'tar -xf {localised_exomiser} -C $BATCH_TMPDIR')
            exomiser_argument = f'--exomiser "${{BATCH_TMPDIR}}/exomiser_{dataset.name}.ht" '
        except ValueError:
            get_logger().info(f'No exomiser results found for {dataset.name}, skipping')
            exomiser_argument = ' '

        # find the clinvar table, localise, and expand
        clinvar_tar = get_clinvar_table()
        localised_clinvar = get_batch().read_input(clinvar_tar)
        job.command(f'tar -xzf {localised_clinvar} -C $BATCH_TMPDIR')

        # read in the massive MT, and unpack it
        localised_mt = get_batch().read_input(input_mt)

        # this will eventually be <DATASET>.tar, but for now detect manually
        mt_name = input_mt.split('/')[-1].removesuffix('.tar')
        job.command(f'tar -xf {localised_mt} -C $BATCH_TMPDIR && rm {localised_mt}')

        job.command(f'export TALOS_CONFIG={conf_in_batch}')
        job.command(
            'RunHailFiltering '
            f'--input "${{BATCH_TMPDIR}}/{mt_name}" '
            f'--panelapp {panelapp_json} '
            f'--pedigree {local_ped} '
            f'--output {job.output["vcf.bgz"]} '
            f'--clinvar "${{BATCH_TMPDIR}}/clinvarbitration_data/clinvar_decisions.ht" '
            f'--pm5 "${{BATCH_TMPDIR}}/clinvarbitration_data/clinvar_decisions.pm5.ht" '
            f'--checkpoint "${{BATCH_TMPDIR}}/checkpoint.mt" '
            f'{exomiser_argument} ',
        )
        get_batch().write_output(job.output, str(expected_out).removesuffix('.vcf.bgz'))

        return self.make_outputs(dataset, data=expected_out, jobs=job)


@stage(required_stages=[QueryPanelapp, GeneratePED, MakeRuntimeConfig])
class RunHailFilteringSV(DatasetStage):
    """
    hail job to filter & label the SV MT
    """

    def expected_outputs(self, dataset: Dataset) -> Path:
        if query_for_sv_vcf(dataset.name):
            return dataset.prefix() / get_date_folder() / 'labelled_SVs.vcf.bgz'
        return {}

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:
        expected_out = self.expected_outputs(dataset)
        runtime_config = str(inputs.as_path(dataset, MakeRuntimeConfig))
        conf_in_batch = get_batch().read_input(runtime_config)
        panelapp_json = get_batch().read_input(str(inputs.as_path(target=dataset, stage=QueryPanelapp)))
        pedigree = inputs.as_path(target=dataset, stage=GeneratePED)
        local_ped = get_batch().read_input(str(pedigree))

        required_storage: int = config_retrieve(['RunHailFiltering', 'storage', 'sv'], 10)
        required_cpu: int = config_retrieve(['RunHailFiltering', 'cores', 'sv'], 2)

        if not (path_or_none := query_for_sv_vcf(dataset.name)):
            get_logger().info(f'No SV MT found for {dataset.name}, skipping')
            return self.make_outputs(dataset, data=expected_out)

        job = get_batch().new_job(f'Run hail SV labelling: {dataset.name}, {path_or_none}')
        # manually extract the VCF and index
        job.declare_resource_group(
            output={
                'vcf.bgz': '{root}.vcf.bgz',
                'vcf.bgz.tbi': '{root}.vcf.bgz.tbi',
            },
        )
        job.image(image_path('talos'))
        # generally runtime under 10 minutes
        job.timeout(config_retrieve(['RunHailFiltering', 'timeouts', 'sv'], 3600))

        # use the new config file
        STANDARD.set_resources(job, ncpu=required_cpu, storage_gb=required_storage, mem_gb=16)

        # get the MANE json file - used to map gene Symbols <-> IDs
        mane_json = get_batch().read_input(config_retrieve(['references', 'mane_1.4', 'json']))

        # copy the VCF in
        annotated_vcf = get_batch().read_input_group(
            **{
                'vcf.bgz': path_or_none,
                'vcf.bgz.tbi': f'{path_or_none}.tbi',
            },
        )['vcf.bgz']
        job.command(f'export TALOS_CONFIG={conf_in_batch}')
        job.command(
            'RunHailFilteringSV '
            f'--input {annotated_vcf} '
            f'--panelapp {panelapp_json} '
            f'--pedigree {local_ped} '
            f'--mane_json {mane_json} '
            f'--output {job.output["vcf.bgz"]} ',  # type: ignore
        )
        get_batch().write_output(job.output, str(expected_out).removesuffix('.vcf.bgz'))

        return self.make_outputs(dataset, data=expected_out, jobs=job)


@stage(
    required_stages=[
        GeneratePED,
        GeneratePanelData,
        QueryPanelapp,
        RunHailFiltering,
        RunHailFilteringSV,
        MakeRuntimeConfig,
    ],
)
class ValidateMOI(DatasetStage):
    """
    run the labelled VCF -> results JSON stage
    """

    def expected_outputs(self, dataset: Dataset) -> Path:
        return dataset.prefix() / get_date_folder() / 'summary_output.json'

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:
        job = get_batch().new_job(f'Talos summary: {dataset.name}')
        job.cpu(config_retrieve(['talos_stages', 'ValidateMOI', 'cpu'], 2.0)).memory(
            config_retrieve(['talos_stages', 'ValidateMOI', 'memory'], 'highmem'),
        ).storage(config_retrieve(['talos_stages', 'ValidateMOI', 'storage'], '10Gi')).image(image_path('talos'))
        # use the new config file
        runtime_config = str(inputs.as_path(dataset, MakeRuntimeConfig))
        conf_in_batch = get_batch().read_input(runtime_config)

        hpo_panels = get_batch().read_input(str(inputs.as_path(dataset, GeneratePanelData)))
        pedigree = get_batch().read_input(str(inputs.as_path(target=dataset, stage=GeneratePED)))
        hail_inputs = inputs.as_path(dataset, RunHailFiltering)

        # either find a SV vcf, or None
        if query_for_sv_vcf(dataset.name):
            hail_sv_inputs = inputs.as_path(dataset, RunHailFilteringSV)
            labelled_sv_vcf = get_batch().read_input_group(
                **{'vcf.bgz': str(hail_sv_inputs), 'vcf.bgz.tbi': f'{hail_sv_inputs}.tbi'},
            )['vcf.bgz']
            sv_vcf_arg = f'--labelled_sv {labelled_sv_vcf} '
        else:
            sv_vcf_arg = ''

        panel_input = get_batch().read_input(str(inputs.as_path(dataset, QueryPanelapp)))
        labelled_vcf = get_batch().read_input_group(
            **{
                'vcf.bgz': str(hail_inputs),
                'vcf.bgz.tbi': str(hail_inputs) + '.tbi',
            },
        )['vcf.bgz']

        job.command(f'export TALOS_CONFIG={conf_in_batch}')
        job.command(
            'ValidateMOI '
            f'--labelled_vcf {labelled_vcf} '
            f'--output {job.output} '
            f'--panelapp {panel_input} '
            f'--pedigree {pedigree} '
            f'--participant_panels {hpo_panels} '
            f'{sv_vcf_arg}',
        )
        expected_out = self.expected_outputs(dataset)
        get_batch().write_output(job.output, str(expected_out))
        return self.make_outputs(dataset, data=expected_out, jobs=job)


@stage(
    required_stages=[MakeRuntimeConfig, ValidateMOI],
    analysis_type='aip-results',
    analysis_keys=['pheno_annotated', 'pheno_filtered'],
)
class HPOFlagging(DatasetStage):

    def expected_outputs(self, dataset: Dataset) -> dict[str, Path]:
        date_prefix = dataset.prefix() / get_date_folder()
        return {
            'pheno_annotated': date_prefix / 'pheno_annotated_report.json',
            'pheno_filtered': date_prefix / 'pheno_filtered_report.json',
        }

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:

        outputs = self.expected_outputs(dataset)

        phenio_db = get_batch().read_input(config_retrieve(['HPOFlagging', 'phenio_db']))
        gene_to_phenotype = get_batch().read_input(config_retrieve(['HPOFlagging', 'gene_to_phenotype']))

        job = get_batch().new_job(f'Label phenotype matches: {dataset.name}')
        job.cpu(2.0).memory('highmem').image(image_path('talos')).storage('20Gi')

        # use the new config file
        runtime_config = str(inputs.as_path(dataset, MakeRuntimeConfig))
        conf_in_batch = get_batch().read_input(runtime_config)

        results_json = get_batch().read_input(str(inputs.as_path(dataset, ValidateMOI)))

        mane_json = get_batch().read_input(config_retrieve(['references', 'mane_1.4', 'json']))

        job.command(f'export TALOS_CONFIG={conf_in_batch}')
        job.command(
            'HPOFlagging '
            f'--input {results_json} '
            f'--mane_json {mane_json} '
            f'--gen2phen {gene_to_phenotype} '
            f'--phenio {phenio_db} '
            f'--output {job.output} '
            f'--phenout {job.phenout} ',
        )

        get_batch().write_output(job.output, str(outputs['pheno_annotated']))
        get_batch().write_output(job.phenout, str(outputs['pheno_filtered']))

        return self.make_outputs(target=dataset, jobs=job, data=outputs)


@stage(
    required_stages=[
        HPOFlagging,
        QueryPanelapp,
        MakeRuntimeConfig,
    ],
)
class CreateTalosHtml(DatasetStage):
    def expected_outputs(self, dataset: Dataset) -> Path:
        return dataset.prefix() / get_date_folder() / 'reports.tar.gz'

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:
        job = get_batch().new_job(f'Talos HTML: {dataset.name}')
        job.image(image_path('talos')).memory('standard').cpu(1.0)

        # use the new config file
        runtime_config = str(inputs.as_path(dataset, MakeRuntimeConfig))
        conf_in_batch = get_batch().read_input(runtime_config)

        results_json = get_batch().read_input(str(inputs.as_dict(dataset, HPOFlagging)['pheno_annotated']))
        panel_input = get_batch().read_input(str(inputs.as_path(dataset, QueryPanelapp)))
        expected_out = self.expected_outputs(dataset)

        # this will write output files directly to GCP
        # report splitting is arbitrary, so can't be reliably done in Hail
        job.command(f'export TALOS_CONFIG={conf_in_batch}')

        # create a new directory for the results
        job.command('mkdir html_outputs')
        job.command('cd html_outputs')

        command_string = (
            f'CreateTalosHTML --input {results_json} --panelapp {panel_input} --output summary_output.html '
        )

        if report_splitting := config_retrieve(['workflow', 'report_splitting', dataset.name], False):
            command_string += f' --split_samples {report_splitting}'

        job.command(command_string)

        # Create a tar'chive here, then use an image with GCloud to copy it up in a bit
        job.command(f'tar -czf {job.output} *')

        get_batch().write_output(job.output, str(expected_out))

        return self.make_outputs(dataset, data=expected_out, jobs=job)


@stage(
    required_stages=[CreateTalosHtml],
    analysis_type='aip-report',
    analysis_keys=['results_html'],
    tolerate_missing_output=True,
)
class UploadTalosHtml(DatasetStage):
    def expected_outputs(self, dataset: Dataset) -> dict[str, str | Path]:
        date_folder_prefix = dataset.prefix(category='web') / get_date_folder()
        return {
            'results_html': date_folder_prefix / 'summary_output.html',
            'folder': str(date_folder_prefix),
        }

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:
        job = get_batch().new_job(f'UploadTalosHtml: {dataset.name}')
        job.image(config_retrieve(['workflow', 'driver_image'])).memory('standard').cpu(1.0)

        input_tarchive = get_batch().read_input(str(inputs.as_path(dataset, CreateTalosHtml)))

        expected_out = self.expected_outputs(dataset)

        authenticate_cloud_credentials_in_job(job)

        # create a new directory for the results
        job.command('mkdir html_outputs')
        job.command(f'tar -xf {input_tarchive} -C html_outputs')
        job.command('cd html_outputs')
        job.command(f'gcloud storage cp -r * {expected_out["folder"]}')

        return self.make_outputs(dataset, data=expected_out, jobs=job)


@stage(
    required_stages=[ValidateMOI, MakeRuntimeConfig],
    analysis_keys=['seqr_file', 'seqr_pheno_file'],
    analysis_type='custom',
    tolerate_missing_output=True,
)
class MinimiseOutputForSeqr(DatasetStage):
    """
    takes the results file from Seqr and produces a minimised form for Seqr ingestion
    """

    def expected_outputs(self, dataset: Dataset) -> dict[str, Path]:
        analysis_folder_prefix = dataset.prefix(category='analysis') / 'seqr_files'
        return {
            'seqr_file': analysis_folder_prefix / f'{get_date_folder()}_seqr.json',
            'seqr_pheno_file': analysis_folder_prefix / f'{get_date_folder()}_seqr_pheno.json',
        }

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:
        # pull out the config section relevant to this datatype & cohort
        # if it doesn't exist for this sequencing type, fail gracefully
        seq_type = config_retrieve(['workflow', 'sequencing_type'])
        try:
            seqr_lookup = config_retrieve(['cohorts', dataset.name, seq_type, 'seqr_lookup'])
        except ConfigError:
            get_logger().warning(f'No Seqr lookup file for {dataset.name} {seq_type}')
            return self.make_outputs(dataset, skipped=True)

        input_localised = get_batch().read_input(str(inputs.as_path(dataset, ValidateMOI)))

        # create a job to run the minimisation script
        job = get_batch().new_job(f'Talos Prep for Seqr: {dataset.name}')
        job.image(image_path('talos')).cpu(1.0).memory('lowmem')

        # use the new config file
        runtime_config = str(inputs.as_path(dataset, MakeRuntimeConfig))
        conf_in_batch = get_batch().read_input(runtime_config)

        lookup_in_batch = get_batch().read_input(seqr_lookup)
        job.command(f'export TALOS_CONFIG={conf_in_batch}')
        job.command(
            'MinimiseOutputForSeqr '
            f'--input {input_localised} '
            f'--output {job.out_json} '
            f'--pheno {job.pheno_json} '
            f'--external_map {lookup_in_batch}',
        )

        # write the results out
        expected_out = self.expected_outputs(dataset)
        get_batch().write_output(job.out_json, str(expected_out['seqr_file']))
        get_batch().write_output(job.pheno_json, str(expected_out['seqr_pheno_file']))
        return self.make_outputs(dataset, data=expected_out, jobs=job)
