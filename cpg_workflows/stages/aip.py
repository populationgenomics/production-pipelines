"""
This is a central script for the stages associated with AIP

Due to privacy concerns, any configuration elements specific to an individual
project are stored in a separate config file, which is in a private repository.
 - production-pipelines-configuration/blob/main/config/aip/aip.toml

The cohort/project specific elements are described at the bottom of
the AIP default config file here:
 - production-pipelines/blob/main/configs/default/aip.toml

[cohorts.DATASET_NAME]
- cohort_panels, list[int], PanelApp panel IDs to apply to this project. By
  default, only the Mendeliome (137) is applied to all analyses.
- gene_prior, str, path to gene prior file to use for this project. When
  identifying Cat 2 (new disease-gene associations), the gene prior is used
  to determine whether a gene is novel or not. In ongoing analysis the gene
  prior used is the PanelApp result from the previous analysis.
- clinvar_filter, list[str], any clinvar submitters to filter out
  for this dataset (to blind this AIP run to its own clinvar entries).
  Note, this is not currently included in this workflow

[cohorts.DATASET_NAME.genome]  # repeated for exome if appropriate
- DATASET_NAME is taken from config[workflow][dataset]
- historic_results, str, path to historic results directory for this project. If
  present, new genes and new variant categories will be identified based on the
  state from the previous run. If absent, panel searching will default to using
  gene_prior above, and the results from this current run will not be stored in
  a suitable format to inform the next run.
- seqr_instance, str, URL for this seqr instance. Remove if not in seqr
- seqr_project, str, project ID for this project/seq type. Remove if not in seqr
- seqr_lookup, str, path to JSON file containing the mapping of CPG internal IDs
  to Seqr IDs, as generated by aip/helpers/process_seqr_metadata.py. If a new Seqr
  load has been performed, this file will need to be updated. This is stored in
  the test bucket so that it can be overwritten by a user. Remove if cohort is
  not in Seqr

Takes as input:
    - Annotated MT path (either found in metamist or directly in config)
    - HPO.obo file (read from cpg-common reference location)
    - Seqr<->Internal ID mapping file (if appropriate, from Config)
Generates:
    - PED file for this cohort
    - Latest panels found through HPO matches against this cohort
    - PanelApp results
    - Category-labelled VCF
    - AIP results JSON (metamist `aip-results` analysis)
    - AIP report HTML (metamist `aip-report` analysis)

This will have to be run using Full permissions as we will need to reference
data in test and main buckets.
"""
import logging
from datetime import datetime
from os.path import join
from functools import lru_cache

from cpg_utils import Path
from cpg_utils.config import get_config
from cpg_utils.hail_batch import (
    authenticate_cloud_credentials_in_job,
    copy_common_env,
    get_batch,
    image_path,
)
from metamist.graphql import gql

from cpg_workflows.metamist import gql_query_optional_logging
from cpg_workflows.resources import STANDARD
from cpg_workflows.workflow import (
    Dataset,
    DatasetStage,
    StageInput,
    StageOutput,
    stage,
)


CHUNKY_DATE = datetime.now().strftime('%Y-%m-%d')
DATED_FOLDER = join('reanalysis', CHUNKY_DATE)
MTA_QUERY = gql(
    """
    query MyQuery($dataset: String!, $type: String!) {
        project(name: $dataset) {
            analyses(type: {eq: $type}, status: {eq: COMPLETED}) {
                output
                timestampCompleted
                meta
            }
        }
    }
"""
)


@lru_cache(maxsize=None)
def query_for_sv_mt(dataset: str, type: str = 'sv') -> str | None:
    """
    query for the latest SV MT for a dataset

    Args:
        dataset (str): project to query for
        type (str): type of analysis entry to query for

    Returns:
        str, the path to the latest MT for the given type
    """

    # hot swapping to a string we can freely modify
    query_dataset = dataset

    if (
        get_config()['workflow'].get('access_level') == 'test'
        and 'test' not in query_dataset
    ):
        query_dataset += '-test'

    result = gql_query_optional_logging(
        MTA_QUERY, query_params={'dataset': query_dataset, 'type': type}
    )
    mt_by_date = {}
    for analysis in result['project']['analyses']:
        if (
            analysis['output']
            and analysis['output'].endswith('.mt')
            and (
                analysis['meta']['sequencing_type']
                == get_config()['workflow'].get('sequencing_type')
            )
        ):
            mt_by_date[analysis['timestampCompleted']] = analysis['output']

    # perfectly acceptable to not have an input SV MT
    if not mt_by_date:
        return None

    # return the latest, determined by a sort on timestamp
    # 2023-10-10... > 2023-10-09..., so sort on strings
    return mt_by_date[sorted(mt_by_date)[-1]]


@lru_cache(maxsize=None)
def query_for_latest_mt(dataset: str, type: str = 'custom') -> str:
    """
    query for the latest MT for a dataset
    Args:
        dataset (str): project to query for
        type (str): type of analysis entry to query for
    Returns:
        str, the path to the latest MT for the given type
    """

    # hot swapping to a string we can freely modify
    query_dataset = dataset

    if (
        get_config()['workflow'].get('access_level') == 'test'
        and 'test' not in query_dataset
    ):
        query_dataset += '-test'
    result = gql_query_optional_logging(
        MTA_QUERY, query_params={'dataset': query_dataset, 'type': type}
    )
    mt_by_date = {}

    for analysis in result['project']['analyses']:
        if (
            analysis['output']
            and analysis['output'].endswith('.mt')
            and (
                analysis['meta']['sequencing_type']
                == get_config()['workflow'].get('sequencing_type')
            )
        ):
            mt_by_date[analysis['timestampCompleted']] = analysis['output']

    if not mt_by_date:
        raise ValueError(f'No MT found for dataset {query_dataset}')

    # return the latest, determined by a sort on timestamp
    # 2023-10-10... > 2023-10-09..., so sort on strings
    return mt_by_date[sorted(mt_by_date)[-1]]


@stage
class GeneratePanelData(DatasetStage):
    """
    PythonJob to find HPO-matched panels
    """

    def expected_outputs(self, dataset: Dataset) -> dict:
        """
        only one output, the panel data
        """
        return {'hpo_panels': dataset.prefix() / DATED_FOLDER / 'hpo_panel_data.json'}

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:

        job = get_batch().new_job(f'Find HPO-matched Panels: {dataset.name}')
        job.cpu(0.25).memory('lowmem')
        job.image(image_path('aip'))

        # auth and copy env
        authenticate_cloud_credentials_in_job(job)
        copy_common_env(job)

        expected_out = self.expected_outputs(dataset)

        query_dataset = dataset.name
        if (
            get_config()['workflow'].get('access_level') == 'test'
            and 'test' not in query_dataset
        ):
            query_dataset += '-test'
        hpo_file = get_batch().read_input(get_config()['workflow']['obo_file'])

        job.command(
            f'python3 reanalysis/hpo_panel_match.py '
            f'--dataset "{query_dataset}" '
            f'--hpo "{hpo_file}" '
            f'--out "{str(expected_out["hpo_panels"])}" '
        )

        return self.make_outputs(dataset, data=expected_out, jobs=job)


@stage(required_stages=[GeneratePanelData])
class QueryPanelapp(DatasetStage):
    """
    query PanelApp for up-to-date gene lists
    """

    def expected_outputs(self, dataset: Dataset) -> dict[str, Path]:
        return {'panel_data': dataset.prefix() / DATED_FOLDER / 'panelapp_data.json'}

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:

        job = get_batch().new_job(f'Query PanelApp: {dataset.name}')
        job.cpu(0.25).memory('lowmem')
        job.image(image_path('aip'))

        # auth and copy env
        authenticate_cloud_credentials_in_job(job)
        copy_common_env(job)

        hpo_panel_json = inputs.as_path(
            target=dataset, stage=GeneratePanelData, key='hpo_panels'
        )
        expected_out = self.expected_outputs(dataset)
        job.command(
            f'python3 reanalysis/query_panelapp.py '
            f'--panels "{str(hpo_panel_json)}" '
            f'--out_path "{str(expected_out["panel_data"])}" '
            f'--dataset {dataset.name} '
        )

        return self.make_outputs(dataset, data=expected_out, jobs=job)


@stage(required_stages=[QueryPanelapp])
class RunHailFiltering(DatasetStage):
    """
    hail job to filter & label the MT
    """

    def expected_outputs(self, dataset: Dataset) -> dict[str, Path]:
        return {
            'labelled_vcf': dataset.prefix() / DATED_FOLDER / 'hail_labelled.vcf.bgz',
            'pedigree': dataset.prefix() / DATED_FOLDER / 'pedigree.ped',
        }

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:

        # either do as you're told, or find the latest
        # this could potentially follow on from the AnnotateDataset stage
        # if this becomes integrated in the main pipeline
        input_mt = get_config()['workflow'].get(
            'matrix_table', query_for_latest_mt(dataset.name)
        )
        job = get_batch().new_job(f'Run hail labelling: {dataset.name}')
        job.image(image_path('aip'))
        STANDARD.set_resources(job, ncpu=1, storage_gb=4)

        # auth and copy env
        authenticate_cloud_credentials_in_job(job)
        copy_common_env(job)

        panelapp_json = inputs.as_path(
            target=dataset, stage=QueryPanelapp, key='panel_data'
        )
        expected_out = self.expected_outputs(dataset)
        pedigree = dataset.write_ped_file(out_path=expected_out['pedigree'])
        # peddy can't read cloud paths
        local_ped = get_batch().read_input(str(pedigree))

        job.command(
            f'python3 reanalysis/hail_filter_and_label.py '
            f'--mt "{input_mt}" '
            f'--panelapp "{panelapp_json}" '
            f'--pedigree "{local_ped}" '
            f'--vcf_out "{str(expected_out["labelled_vcf"])}" '
            f'--dataset {dataset.name} '
        )

        return self.make_outputs(dataset, data=expected_out, jobs=job)


@stage(required_stages=[QueryPanelapp])
class RunHailSVFiltering(DatasetStage):
    """
    hail job to filter & label the SV MT
    """

    def expected_outputs(self, dataset: Dataset) -> dict[str, Path]:
        return {
            'labelled_vcf': dataset.prefix()
            / DATED_FOLDER
            / 'SV_hail_labelled.vcf.bgz',
            'pedigree': dataset.prefix() / DATED_FOLDER / 'pedigree.ped',
        }

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:

        expected_out = self.expected_outputs(dataset)
        sv_mt = get_config()['workflow'].get(
            'sv_matrix_table', query_for_sv_mt(dataset.name)
        )

        # this might work? May require some config entries
        if sv_mt is None:
            logging.warning(f'No SV MT found for {dataset.name}, skipping stage')
            return self.make_outputs(dataset, data=expected_out, jobs=[], skipped=True)

        job = get_batch().new_job(f'Run hail SV labelling: {dataset.name}')
        job.image(image_path('aip'))
        STANDARD.set_resources(job, ncpu=1, storage_gb=4)

        # auth and copy env
        authenticate_cloud_credentials_in_job(job)
        copy_common_env(job)

        panelapp_json = inputs.as_path(
            target=dataset, stage=QueryPanelapp, key='panel_data'
        )
        pedigree = dataset.write_ped_file(out_path=expected_out['pedigree'])
        # peddy can't read cloud paths
        local_ped = get_batch().read_input(str(pedigree))

        job.command(
            f'python3 reanalysis/hail_filter_sv.py '
            f'--mt "{sv_mt}" '
            f'--panelapp "{panelapp_json}" '
            f'--pedigree "{local_ped}" '
            f'--vcf_out "{str(expected_out["labelled_vcf"])}" '
        )

        return self.make_outputs(dataset, data=expected_out, jobs=job)


def _aip_summary_meta(
    output_path: str,  # pylint: disable=W0613:unused-argument
) -> dict[str, str]:
    """
    Add meta.type to custom analysis object
    """
    return {'type': 'aip_output_json'}


@stage(
    required_stages=[
        GeneratePanelData,
        QueryPanelapp,
        RunHailFiltering,
        RunHailSVFiltering,
    ],
    analysis_type='aip-results',
    analysis_keys=['summary_json'],
    update_analysis_meta=_aip_summary_meta,
)
class ValidateMOI(DatasetStage):
    """
    run the labelled VCF -> results JSON stage
    """

    def expected_outputs(self, dataset: Dataset) -> dict[str, Path]:
        return {'summary_json': dataset.prefix() / DATED_FOLDER / 'summary_output.json'}

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:
        job = get_batch().new_job(f'AIP summary: {dataset.name}')
        job.cpu(1.0).memory('standard')

        # auth and copy env
        job.image(image_path('aip'))
        authenticate_cloud_credentials_in_job(job)
        copy_common_env(job)
        hpo_panels = str(inputs.as_dict(dataset, GeneratePanelData)['hpo_panels'])
        hail_inputs = inputs.as_dict(dataset, RunHailFiltering)

        input_path = get_config()['workflow'].get(
            'matrix_table', query_for_latest_mt(dataset.name)
        )

        # the SV vcf is accepted, but is not always generated
        sv_vcf_arg = ''
        if sv_path := get_config()['workflow'].get(
            'sv_matrix_table', query_for_sv_mt(dataset.name)
        ):
            # bump input_path to contain both source files if appropriate
            input_path = f'{input_path}, {sv_path}'
            hail_sv_inputs = inputs.as_dict(dataset, RunHailSVFiltering)
            labelled_sv_vcf = get_batch().read_input_group(
                **{
                    'vcf.bgz': str(hail_sv_inputs['labelled_vcf']),
                    'vcf.bgz.tbi': f'{hail_sv_inputs["labelled_vcf"]}.tbi',
                }
            )['vcf.bgz']
            sv_vcf_arg = f'--labelled_sv "{labelled_sv_vcf}" '

        panel_input = str(inputs.as_dict(dataset, QueryPanelapp)['panel_data'])
        # peddy can't read cloud paths
        local_ped = get_batch().read_input(str(hail_inputs['pedigree']))
        labelled_vcf = get_batch().read_input_group(
            **{
                'vcf.bgz': str(hail_inputs['labelled_vcf']),
                'vcf.bgz.tbi': str(hail_inputs['labelled_vcf']) + '.tbi',
            }
        )['vcf.bgz']
        out_json_path = str(self.expected_outputs(dataset)['summary_json'])
        job.command(
            f'python3 reanalysis/validate_categories.py '
            f'--labelled_vcf "{labelled_vcf}" '
            f'--out_json "{out_json_path}" '
            f'--panelapp "{panel_input}" '
            f'--pedigree "{local_ped}" '
            f'--input_path "{input_path}" '
            f'--participant_panels "{hpo_panels}" '
            f'--dataset "{dataset.name}" {sv_vcf_arg}'
        )
        expected_out = self.expected_outputs(dataset)
        return self.make_outputs(dataset, data=expected_out, jobs=job)


def _aip_html_meta(
    output_path: str,  # pylint: disable=W0613:unused-argument
) -> dict[str, str]:
    """
    Add meta.type to custom analysis object
    This isn't quite conformant with what AIP alone produces
    e.g. doesn't have the full URL to the results in GCP
    """
    return {'type': 'aip_output_html'}


@stage(
    required_stages=[ValidateMOI, QueryPanelapp, RunHailFiltering],
    analysis_type='aip-report',
    analysis_keys=['results_html', 'latest_html'],
    update_analysis_meta=_aip_html_meta,
    tolerate_missing_output=True,
)
class CreateAIPHTML(DatasetStage):
    def expected_outputs(self, dataset: Dataset) -> dict[str, Path]:
        return {
            'results_html': dataset.prefix(category='web')
            / DATED_FOLDER
            / 'summary_output.html',
            'latest_html': dataset.prefix(category='web')
            / DATED_FOLDER
            / f'summary_latest_{CHUNKY_DATE}.html',
        }

    def queue_jobs(self, dataset: Dataset, inputs: StageInput) -> StageOutput:
        job = get_batch().new_job(f'AIP HTML: {dataset.name}')
        job.cpu(1.0).memory('lowmem')
        job.image(image_path('aip'))

        # auth and copy env
        authenticate_cloud_credentials_in_job(job)
        copy_common_env(job)

        moi_inputs = inputs.as_dict(dataset, ValidateMOI)['summary_json']
        panel_input = inputs.as_dict(dataset, QueryPanelapp)['panel_data']
        pedigree = inputs.as_dict(dataset, RunHailFiltering)['pedigree']
        local_ped = get_batch().read_input(str(pedigree))

        expected_out = self.expected_outputs(dataset)
        job.command(
            f'python3 reanalysis/html_builder.py '
            f'--results "{moi_inputs}" '
            f'--panelapp "{panel_input}" '
            f'--pedigree "{local_ped}" '
            f'--output "{expected_out["results_html"]}" '
            f'--latest "{expected_out["latest_html"]}" '
            f'--dataset "{dataset.name}" '
        )

        expected_out = self.expected_outputs(dataset)
        return self.make_outputs(dataset, data=expected_out, jobs=job)
